{
 "metadata": {
  "name": "",
  "signature": "sha256:13da0b9a977656b41c1239edb9404a8bf90e0b2348dcb5246c38b3f54aee4254"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Text Mining the Erowid Experience Vault"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "import packages and set path"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os, sys, re, math\n",
      "path = os.path.abspath(\"\")+\"/\"\n",
      "print path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/glenn/Documents/projects/mining-erowid/\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "generator to iterate files into memory as lists of words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, nltk.tokenize, nltk.corpus\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def is_number(s):\n",
      "    try:\n",
      "        float(s)\n",
      "        return True\n",
      "    except ValueError:\n",
      "        return False\n",
      "\n",
      "def is_okay_word(s):\n",
      "    import re\n",
      "    if len(s)==1:\n",
      "        return False\n",
      "    elif is_number(s) and float(s)<1900:\n",
      "        return False\n",
      "    elif re.match('\\d+[mM]?[gGlLxX]',s):\n",
      "        return False\n",
      "    elif re.match('\\d+[oO][zZ]',s):\n",
      "        return False\n",
      "    else:\n",
      "        return True\n",
      "\n",
      "def yield_body_text(path):\n",
      "    stopwords = nltk.corpus.stopwords.words('english')\n",
      "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
      "    tagger = nltk.tag.UnigramTagger(nltk.corpus.brown.tagged_sents())\n",
      "    lemmatizer = nltk.WordNetLemmatizer()\n",
      "    from nltk.corpus import wordnet\n",
      "    experiences = os.listdir(path+'xml')\n",
      "    for n, experience in enumerate(experiences):\n",
      "        #if n<9900:\n",
      "            #continue\n",
      "        #if n>10000:\n",
      "            #return\n",
      "        words = []\n",
      "        with open(path+\"xml/\"+experience) as file:\n",
      "            soup = BeautifulSoup(file)\n",
      "            #print experience\n",
      "            tokens = tokenizer.tokenize(soup.bodytext.contents[0])\n",
      "            pos = tagger.tag(tokens)\n",
      "            words = []\n",
      "            for token in pos:\n",
      "                if token[1] == 'NN':\n",
      "                    pos = wordnet.NOUN\n",
      "                elif token[1] == 'JJ':\n",
      "                    pos = wordnet.ADJ\n",
      "                elif token[1] == 'VB':\n",
      "                    pos = wordnet.VERB\n",
      "                elif token[1] == 'RV':\n",
      "                    pos = wordnet.ADV\n",
      "                else:\n",
      "                    pos = wordnet.NOUN\n",
      "                lemma = lemmatizer.lemmatize(token[0], pos)\n",
      "                if is_okay_word(lemma) and lemma not in stopwords:\n",
      "                    words.append(lemma)\n",
      "        if n%1000==0:\n",
      "            print(\"Finished \" + str(n) + \" files out of \" + str(len(experiences)))\n",
      "        yield \" \".join(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "create the bag-of-words using CountVectorizer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "cv = CountVectorizer(min_df=2)\n",
      "bag = cv.fit_transform(yield_body_text(path))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished 0 files out of 19634\n",
        "Finished 1000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 2000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 3000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 4000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 5000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 6000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 7000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 8000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 9000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 10000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 11000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 12000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 13000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 14000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 15000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 16000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 17000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 18000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 19000 files out of 19634"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Save or load the serialized matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "pickle.dump(bag, open(\"bag.p\",\"wb\"))\n",
      "#bag = pickle.load(open(\"bag.p\",\"rb\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Determine feature weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "tf = TfidfTransformer()\n",
      "tfidf_bag = tf.fit_transform(bag)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Model topics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import TruncatedSVD\n",
      "svd = TruncatedSVD(n_components=100)\n",
      "svd_bag = svd.fit_transform(tfidf_bag)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 149
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Get lists of important terms for each topic"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_topic_names(model, vzr):\n",
      "    topics = []\n",
      "    for i in range(np.shape(model.components_)[0]):\n",
      "        weights = list(model.components_[i])\n",
      "        z = zip(weights,vzr.get_feature_names())\n",
      "        z.sort(reverse=True)\n",
      "        unzip = zip(*z)\n",
      "        topics.append(unzip[1])\n",
      "    return topics\n",
      "\n",
      "topics = get_topic_names(svd,cv)\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    }
   ],
   "metadata": {}
  }
 ]
}